{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def txt_to_csv(input_file, output_file):\n",
    "    with open(input_file, 'r') as txt_file:\n",
    "        # 读取txt文件的内容\n",
    "        lines = txt_file.readlines()\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        # 创建CSV写入器\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # 写入CSV文件\n",
    "        for line in lines:\n",
    "            # 假设每行内容都是用逗号分隔的\n",
    "            row = line.strip().split(',')\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_txt_file = \"source_data/IWIN_Weather_AgERA5_20210211.txt\"\n",
    "    output_csv_file = \"output/IWIN_Weather_AgERA5_20210211.csv\"\n",
    "\n",
    "    txt_to_csv(input_txt_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/IWIN_Weather_AgERA5_20210211.csv')\n",
    "\n",
    "df_filtered = df[df['Year'] >= 2002]\n",
    "\n",
    "df_filtered.to_csv('output/IWIN_Weather_AgERA5_2003-2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "weather_data_file = 'output/IWIN_Weather_AgERA5_2003-2021.csv'\n",
    "merged_data_file = '../1_Pheno/output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv'\n",
    "\n",
    "weather_data = pd.read_csv(weather_data_file)\n",
    "merged_data = pd.read_csv(merged_data_file)\n",
    "\n",
    "unique_locations = merged_data['Loc_no'].drop_duplicates()\n",
    "\n",
    "filtered_weather_data = weather_data[weather_data['location'].isin(unique_locations)]\n",
    "\n",
    "filtered_weather_data.to_csv('output/IWIN_Weather_AgERA5_2003-2021_Trimed.csv', index=False)\n",
    "\n",
    "print(f\"筛选完成！共保留 {len(filtered_weather_data)} 行数据，涉及 {len(unique_locations)} 个唯一地点。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "weather_data_file = 'output/IWIN_Weather_AgERA5_2003-2021_Trimed.csv'\n",
    "merged_data_file = '../1_Pheno/output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv'\n",
    "\n",
    "weather_data = pd.read_csv(weather_data_file)\n",
    "merged_data = pd.read_csv(merged_data_file)\n",
    "\n",
    "def combine_date(year, month, day):\n",
    "    return pd.to_datetime({'year': year, 'month': month, 'day': day})\n",
    "\n",
    "merged_data['SowDate'] = combine_date(\n",
    "    merged_data['SowYear'], merged_data['SowMonth'], merged_data['SowDay']\n",
    ")\n",
    "merged_data['HarDate'] = combine_date(\n",
    "    merged_data['HarYear'], merged_data['HarMonth'], merged_data['HarDay']\n",
    ")\n",
    "\n",
    "weather_data['Date'] = pd.to_datetime(\n",
    "    weather_data[['Year', 'Month', 'Day']]\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in merged_data.iterrows():\n",
    "    print(index)\n",
    "    loc_no = row['Loc_no']\n",
    "    sow_date = row['SowDate']\n",
    "    har_date = row['HarDate']\n",
    "    days_before_sow = sow_date - timedelta(days=14)\n",
    "\n",
    "    weather_subset = weather_data[\n",
    "        (weather_data['location'] == loc_no) &\n",
    "        (weather_data['Date'] >= days_before_sow) &\n",
    "        (weather_data['Date'] <= har_date)\n",
    "    ]\n",
    "\n",
    "    if weather_subset.empty:\n",
    "        print(f\"Warning: No weather data found for Loc_no={loc_no}, SowDate={sow_date}, HarvestDate={har_date}\")\n",
    "        continue\n",
    "\n",
    "    weather_subset = weather_subset.sort_values('Date')\n",
    "    weather_subset['WeekIndex'] = (weather_subset['Date'] - days_before_sow).dt.days // 7\n",
    "    weekly_weather = weather_subset.groupby('WeekIndex').mean(numeric_only=True).round(2)\n",
    "\n",
    "    selected_weather_columns = [\n",
    "        'Precipitation [mm]', 'Relative Humidity max [%]', \n",
    "        'Relative Humidity min [%]', 'Shortwave Radiation [MJ/m2/d]', \n",
    "        'TemperatureMax [C]', 'TemperatureMin [C]', \n",
    "        'Vapor Pressure Deficit max [kPa]', \n",
    "        'Wind Speed 2m [m/s]', 'Wind Speed 10m [m/s]'\n",
    "    ]\n",
    "\n",
    "    flattened_data = {}\n",
    "    for week_idx, week_data in weekly_weather.iterrows():\n",
    "        for col in selected_weather_columns:\n",
    "            flattened_data[f\"Week{week_idx+1}_{col}\"] = week_data[col]\n",
    "\n",
    "    results.append({**row.to_dict(), **flattened_data})\n",
    "\n",
    "final_df = pd.DataFrame(results)\n",
    "final_df.to_csv('output/PhWeeklyWeather.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "data = pd.read_csv('output/YieldWeeklyWeather.csv')\n",
    "\n",
    "start_column = \"Week1_Precipitation [mm]\"\n",
    "subset_data = data.loc[:, start_column:]\n",
    "\n",
    "subset_data = subset_data.fillna(0)\n",
    "\n",
    "processed_data = []\n",
    "for index, row in subset_data.iterrows():\n",
    "\n",
    "    row_data = row.dropna().values\n",
    "    if len(row_data) > 0:\n",
    "        if len(row_data) % 9 == 0:\n",
    "            row_matrix = np.array(row_data).reshape(-1, 9)\n",
    "            processed_data.append(row_matrix)\n",
    "        else:\n",
    "            print(index+1)\n",
    "            print(row_data.shape)\n",
    "            \n",
    "data_array = np.array(processed_data)\n",
    "\n",
    "with open('output/PhWeeklyWeather.pkl', 'wb') as f:\n",
    "    pickle.dump(data_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "with open('output/YieldWeeklyWeather.pkl', 'rb') as file:\n",
    "    matrix = pickle.load(file)\n",
    "\n",
    "N, W, H = matrix.shape\n",
    "\n",
    "matrix = matrix.reshape(N * W, H)\n",
    "\n",
    "zero_rows = np.all(matrix == 0, axis=1)\n",
    "\n",
    "matrix[zero_rows] = np.nan\n",
    "\n",
    "column_means = np.nanmean(matrix, axis=0)\n",
    "column_stds = np.nanstd(matrix, axis=0)\n",
    "\n",
    "normalized_matrix = np.zeros_like(matrix)\n",
    "\n",
    "for i in range(matrix.shape[1]):\n",
    "    if column_stds[i] != 0.0:\n",
    "        normalized_matrix[:, i] = (matrix[:, i] - column_means[i]) / column_stds[i]\n",
    "\n",
    "normalized_matrix[np.isnan(normalized_matrix)] = 0\n",
    "\n",
    "normalized_matrix = normalized_matrix.reshape(N, W, H)\n",
    "\n",
    "with open('output/PhWeeklyWeatherNormalized.pkl', 'wb') as file:\n",
    "    pickle.dump(normalized_matrix, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIGS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
