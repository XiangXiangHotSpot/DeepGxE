{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Process for Plant Height Phenotype Files**\n",
    "\n",
    "The first step involves concatenating all downloaded plant height phenotypic files (e.g., the “PLNTHT” sheet in 46 IBWSN.xls) into the following format (All.csv).\n",
    "\n",
    "```csv\n",
    "Trial name,Occ,Loc_no,Cycle,Cid,Sid,Value,Gid\n",
    "24 ESWYT,1.0,14002.0,2003,61665,1,97,304660\n",
    "24 ESWYT,1.0,14002.0,2003,8195,5,78,16004\n",
    "24 ESWYT,1.0,14002.0,2003,160278,68,74,3617481\n",
    "24 ESWYT,1.0,14002.0,2003,384555,1,72,2668073\n",
    "24 ESWYT,1.0,14002.0,2003,113388,5,79,1491661\n",
    "24 ESWYT,1.0,14002.0,2003,162515,1,74,217743\n",
    "24 ESWYT,1.0,14002.0,2003,67613,97,79,4057141\n",
    "```\n",
    "\n",
    "Additionally, concatenate all corresponding CID, SID, and GID values into the following format (CidSidGid.csv):\n",
    "\n",
    "```csv\n",
    "CID,SID,GID\n",
    "61665,1,304660\n",
    "60115,215,3820651\n",
    "73574,1908,3829327\n",
    "74933,481,3833626\n",
    "```\n",
    "\n",
    "Combine the rows containing the keywords “SOWING_DATE” and “HARVEST_FINISHING_DATE” from files in the phenotype files that include “EnvData” in their names into two separate files, named SOWING_DATE.csv and HARVEST_FINISHING_DATE.csv, respectively, as follows:\n",
    "\n",
    "```csv\n",
    "Trial name,Occ,Loc_no,Country,Loc_desc,Cycle,Trait No,Trait name,Value,Unit,Year,Month,Day\n",
    "24 ESWYT,1,14002,ANGOLA,HUMPATA,2003,3,SOWING_DATE,Mar 1 2003,date,2003,3,1\n",
    "24 ESWYT,6,11109,ZIMBABWE,CHISIPITE,2003,3,SOWING_DATE,May 8 2003,date,2003,5,8\n",
    "24 ESWYT,7,22620,PAKISTAN,SAKRAND,2003,3,SOWING_DATE,Dec 6 2003,date,2003,12,6\n",
    "```\n",
    "\n",
    "```csv\n",
    "Trial name,Occ,Loc_no,Country,Loc_desc,Cycle,Trait No,Trait name,Value,Unit,Year,Month,Day\n",
    "24 ESWYT,1,14002,ANGOLA,HUMPATA,2003,9,HARVEST_FINISHING_DATE,Jul 23 2003,date,2003,7,23\n",
    "24 ESWYT,6,11109,ZIMBABWE,CHISIPITE,2003,9,HARVEST_FINISHING_DATE,Oct 27 2003,date,2003,10,27\n",
    "24 ESWYT,8,22607,PAKISTAN,NARC ISLAMABAD,2003,9,HARVEST_FINISHING_DATE,Apr 28 2004,date,2004,4,28\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_with_gid = pd.read_csv('source_data/All.csv')\n",
    "\n",
    "all_with_gid = all_with_gid.dropna()\n",
    "\n",
    "all_with_gid['Gid'] = 'GID' + all_with_gid['Gid'].astype(int).astype(str)\n",
    "\n",
    "all_with_gid['Occ'] = all_with_gid['Occ'].astype(int)\n",
    "all_with_gid['Loc_no'] = all_with_gid['Loc_no'].astype(int)\n",
    "\n",
    "all_with_gid.to_csv('output/AllWithGidDropMissingValues.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "allpheno_df = pd.read_csv(\"output/AllWithGidDropMissingValues.csv\")\n",
    "\n",
    "sowing_date_df = pd.read_csv(\"source_data/SOWING_DATE.csv\", dtype={\"Year\": int, \"Month\": int, \"Day\": int})\n",
    "sowing_date_df.drop_duplicates(subset=[\"Occ\", \"Loc_no\", \"Cycle\"], keep=\"first\", inplace=True)\n",
    "\n",
    "sowing_date_dict = sowing_date_df.set_index([\"Occ\", \"Loc_no\", \"Cycle\"])[[\"Year\", \"Month\", \"Day\"]].to_dict(orient=\"index\")\n",
    "\n",
    "def get_date(row):\n",
    "    key = (row[\"Occ\"], row[\"Loc_no\"], row[\"Cycle\"])\n",
    "    date_data = sowing_date_dict.get(key)\n",
    "    if date_data:\n",
    "        return int(date_data[\"Year\"]), int(date_data[\"Month\"]), int(date_data[\"Day\"])\n",
    "    return None, None, None\n",
    "\n",
    "allpheno_df[[\"SowYear\", \"SowMonth\", \"SowDay\"]] = allpheno_df.apply(get_date, axis=1, result_type=\"expand\")\n",
    "\n",
    "allpheno_df.to_csv(\"output/AllWithGidDropMissingValuesSow.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "allpheno_df = pd.read_csv(\"output/AllWithGidDropMissingValuesSow.csv\")\n",
    "\n",
    "sowing_date_df = pd.read_csv(\"source_data/HARVEST_FINISHING_DATE.csv\", dtype={\"Year\": int, \"Month\": int, \"Day\": int})\n",
    "sowing_date_df.drop_duplicates(subset=[\"Occ\", \"Loc_no\", \"Cycle\"], keep=\"first\", inplace=True)\n",
    "\n",
    "sowing_date_dict = sowing_date_df.set_index([\"Occ\", \"Loc_no\", \"Cycle\"])[[\"Year\", \"Month\", \"Day\"]].to_dict(orient=\"index\")\n",
    "\n",
    "def get_date(row):\n",
    "    key = (row[\"Occ\"], row[\"Loc_no\"], row[\"Cycle\"])\n",
    "    date_data = sowing_date_dict.get(key)\n",
    "    if date_data:\n",
    "        return int(date_data[\"Year\"]), int(date_data[\"Month\"]), int(date_data[\"Day\"])\n",
    "    return None, None, None\n",
    "\n",
    "allpheno_df[[\"HarYear\", \"HarMonth\", \"HarDay\"]] = allpheno_df.apply(get_date, axis=1, result_type=\"expand\")\n",
    "\n",
    "allpheno_df = allpheno_df.dropna()\n",
    "\n",
    "allpheno_df[[\"SowYear\", \"SowMonth\", \"SowDay\", \"HarYear\", \"HarMonth\", \"HarDay\"]] = allpheno_df[[\"SowYear\", \"SowMonth\", \"SowDay\", \"HarYear\", \"HarMonth\", \"HarDay\"]].astype(int)\n",
    "\n",
    "allpheno_df['SowDate'] = pd.to_datetime(allpheno_df[['SowYear', 'SowMonth', 'SowDay']].astype(str).agg('-'.join, axis=1))\n",
    "allpheno_df['HarDate'] = pd.to_datetime(allpheno_df[['HarYear', 'HarMonth', 'HarDay']].astype(str).agg('-'.join, axis=1))\n",
    "\n",
    "allpheno_df['Days'] = (allpheno_df['HarDate'] - allpheno_df['SowDate']).dt.days\n",
    "\n",
    "allpheno_df.drop(['SowDate', 'HarDate'], axis=1, inplace=True)\n",
    "\n",
    "allpheno_df.to_csv(\"output/AllWithGidDropMissingValuesSowHar.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as txt_file:\n",
    "        values = txt_file.read().split()\n",
    "    return set(values)\n",
    "\n",
    "def filter_csv_file(input_file, txt_file, output_file):\n",
    "    values_set = read_txt_file(txt_file)\n",
    "\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    filtered_df = df[df['Gid'].isin(values_set)]\n",
    "\n",
    "    filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = \"output/AllWithGidDropMissingValuesSowHar.csv\"\n",
    "    txt_file = \"../2_Geno/output/genotype_ID.txt\" \n",
    "    output_csv_file = \"output/AllWithGidDropMissingValuesSowHarFilteredGid.csv\"\n",
    "\n",
    "    filter_csv_file(input_csv_file, txt_file, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_before2020 = 'output/AllWithGidDropMissingValuesSowHarFilteredGid.csv'\n",
    "file_weather_data = '../3_Env/output/IWIN_Weather_AgERA5_20210211.csv'\n",
    "output_file = 'output/AllWithGidDropMissingValuesSowHarFilteredGidLoc.csv'\n",
    "\n",
    "data_before2020 = pd.read_csv(file_before2020)\n",
    "data_weather = pd.read_csv(file_weather_data)\n",
    "\n",
    "locations_before2020 = set(data_before2020['Loc_no'])\n",
    "locations_weather_data = set(data_weather['location'])\n",
    "\n",
    "cleaned_data = data_before2020[data_before2020['Loc_no'].isin(locations_weather_data)]\n",
    "\n",
    "deleted_data = data_before2020[~data_before2020['Loc_no'].isin(locations_weather_data)]\n",
    "\n",
    "if not deleted_data.empty:\n",
    "    print(f\"Deleted {len(deleted_data)} rows where 'Loc_no' is not in 'location'.\")\n",
    "    deleted_file_name = 'output/deleted_data.csv'\n",
    "    deleted_data.to_csv(deleted_file_name, index=False)\n",
    "    print(f\"Deleted rows saved to file: {deleted_file_name}\")\n",
    "else:\n",
    "    print(\"All 'Loc_no' values exist in 'location'. No rows were deleted.\")\n",
    "\n",
    "if not cleaned_data.empty:\n",
    "    print(f\"Cleaned {len(cleaned_data)} rows where 'Loc_no' exists in 'location'.\")\n",
    "    cleaned_data.to_csv(output_file, index=False)\n",
    "    print(f\"Cleaned data saved to file: {output_file}\")\n",
    "else:\n",
    "    print(\"No rows to clean. All 'Loc_no' values exist in 'location'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLoc.csv')\n",
    "\n",
    "condition = (df['Days'] > 270) | (df['HarYear'] >= 2020 ) | (df['Value'] == \"-\" )\n",
    "\n",
    "filtered_df = df[condition]\n",
    "\n",
    "filtered_df.to_csv('output/Unormal.csv', index=False)\n",
    "\n",
    "df.drop(filtered_df.index, inplace=True)\n",
    "\n",
    "df.to_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormal.csv')\n",
    "\n",
    "df_duplicates = df[df.duplicated(keep='first')]\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "if not df_duplicates.empty:\n",
    "    print(\"The first duplicate row:\")\n",
    "    print(df_duplicates.head())\n",
    "\n",
    "df_duplicates.to_csv('output/Duplicate.csv', index=False)\n",
    "print(\"The removed duplicate rows have been saved to Duplicate.csv.\")\n",
    "\n",
    "df.to_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv', index=False)\n",
    "print(\"The updated data has been saved to AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"提取去除重复后的环境型\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv')\n",
    "\n",
    "df['Occ_Loc_no_Cycle'] = df['Occ'].astype(str) + '_' + df['Loc_no'].astype(str) + '_' + df['Cycle'].astype(str)\n",
    "\n",
    "df = df.drop_duplicates(subset='Occ_Loc_no_Cycle', keep='first')\n",
    "\n",
    "df.to_csv('output/UniqueOccLocCycle.csv', columns=['Occ_Loc_no_Cycle'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"提取去除重复后的基因型\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv')\n",
    "\n",
    "unique_gid_series = df['Gid'].drop_duplicates(keep='first')\n",
    "\n",
    "new_df = pd.DataFrame({'Gid': unique_gid_series})\n",
    "\n",
    "new_df.to_csv('output/UniqueGid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIGS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
