{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Process for Wheat Yield Phenotype Files**\n",
    "\n",
    "The process involves concatenating all downloaded phenotype files (those containing the keyword \"GrnYld\") into the following format (All.csv):\n",
    "\n",
    "```csv\n",
    "Trial name,Occ,Loc_no,Country,Loc_desc,Cycle,Cid,Sid,Gen_name,Trait_no,Trait name,Value,EMS,SE,Unit,GID,Plot\n",
    "24ESWYT,1,14002,ANGOLA,HUMPATA,2003,61665,1,LOCAL CHECK,144,GRAIN_YIELD,2.375,0.018,0.681,t/ha,,\n",
    "24ESWYT,1,14002,ANGOLA,HUMPATA,2003,8195,5,RAYON F 89,144,GRAIN_YIELD,4.029,0.018,0.681,t/ha,,\n",
    "24ESWYT,1,14002,ANGOLA,HUMPATA,2003,160278,68,TARACHI F 2000,144,GRAIN_YIELD,4.134,0.018,0.681,t/ha,,\n",
    "```\n",
    "\n",
    "Additionally, concatenate all corresponding CID, SID, and GID values into the following format (CidSidGid.csv):\n",
    "\n",
    "```csv\n",
    "CID,SID,GID\n",
    "61665,1,304660\n",
    "60115,215,3820651\n",
    "73574,1908,3829327\n",
    "74933,481,3833626\n",
    "```\n",
    "\n",
    "Combine the rows containing the keywords “SOWING_DATE” and “HARVEST_FINISHING_DATE” from files in the phenotype files that include “EnvData” in their names into two separate files, named SOWING_DATE.csv and HARVEST_FINISHING_DATE.csv, respectively, as follows:\n",
    "\n",
    "```csv\n",
    "Trial name,Occ,Loc_no,Country,Loc_desc,Cycle,Trait No,Trait name,Value,Unit,Year,Month,Day\n",
    "24 ESWYT,1,14002,ANGOLA,HUMPATA,2003,3,SOWING_DATE,Mar 1 2003,date,2003,3,1\n",
    "24 ESWYT,6,11109,ZIMBABWE,CHISIPITE,2003,3,SOWING_DATE,May 8 2003,date,2003,5,8\n",
    "24 ESWYT,7,22620,PAKISTAN,SAKRAND,2003,3,SOWING_DATE,Dec 6 2003,date,2003,12,6\n",
    "```\n",
    "\n",
    "```csv\n",
    "Trial name,Occ,Loc_no,Country,Loc_desc,Cycle,Trait No,Trait name,Value,Unit,Year,Month,Day\n",
    "24 ESWYT,1,14002,ANGOLA,HUMPATA,2003,9,HARVEST_FINISHING_DATE,Jul 23 2003,date,2003,7,23\n",
    "24 ESWYT,6,11109,ZIMBABWE,CHISIPITE,2003,9,HARVEST_FINISHING_DATE,Oct 27 2003,date,2003,10,27\n",
    "24 ESWYT,8,22607,PAKISTAN,NARC ISLAMABAD,2003,9,HARVEST_FINISHING_DATE,Apr 28 2004,date,2004,4,28\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv('source_data/All.csv')\n",
    "\n",
    "# Select the required columns\n",
    "selected_columns = ['Trial name', 'Occ', 'Loc_no', 'Cycle', 'Cid', 'Sid', 'Value']\n",
    "\n",
    "selected_df = df[selected_columns]\n",
    "\n",
    "# Save the selected columns to a new CSV file\n",
    "selected_df.to_csv('output/AllSelected.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the AllSelected.CSV file\n",
    "all_selected = pd.read_csv('output/AllSelected.csv')\n",
    "\n",
    "# Read the CidSidGid.csv file\n",
    "cid_sid_gid = pd.read_csv('source_data/CidSidGid.csv')\n",
    "\n",
    "# Merge the two dataframes by matching Cid and Sid, and extract GID\n",
    "merged = pd.merge(\n",
    "    all_selected, \n",
    "    cid_sid_gid, \n",
    "    how='left', \n",
    "    left_on=['Cid', 'Sid'], \n",
    "    right_on=['CID', 'SID']\n",
    ")\n",
    "\n",
    "# Remove redundant CID and SID columns, keeping only the Gid data\n",
    "merged = merged.drop(columns=['CID', 'SID'])\n",
    "\n",
    "# Save the new CSV file\n",
    "merged.to_csv('output/AllWithGid.csv', index=False)\n",
    "\n",
    "print(\"New CSV file has been generated: AllWithGid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_with_gid = pd.read_csv('output/AllWithGid.csv')\n",
    "\n",
    "# Replace empty strings and values with only spaces with NaN\n",
    "all_with_gid = all_with_gid.applymap(lambda x: np.nan if isinstance(x, str) and x.strip() == \"\" else x)\n",
    "\n",
    "# Remove rows containing NaN\n",
    "all_with_gid = all_with_gid.dropna()\n",
    "\n",
    "# Ensure the GID column is renamed to Gid and convert to the correct format\n",
    "all_with_gid = all_with_gid.rename(columns={'GID': 'Gid'})\n",
    "\n",
    "# Format Gid column values as 'GID' followed by the integer value of Gid\n",
    "all_with_gid['Gid'] = 'GID' + all_with_gid['Gid'].astype(int).astype(str)\n",
    "\n",
    "all_with_gid.to_csv('output/AllWithGidDropMissingValues.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "allpheno_df = pd.read_csv(\"output/AllWithGidDropMissingValues.csv\")\n",
    "\n",
    "# Read SOWING_DATE.csv file and handle duplicate rows\n",
    "sowing_date_df = pd.read_csv(\"source_data/SOWING_DATE.csv\", dtype={\"Year\": int, \"Month\": int, \"Day\": int})\n",
    "sowing_date_df.drop_duplicates(subset=[\"Occ\", \"Loc_no\", \"Cycle\"], keep=\"first\", inplace=True)\n",
    "\n",
    "# Convert SOWING_DATE.csv data to a dictionary with (Occ, Loc_no, Cycle) as key and (Year, Month, Day) as values\n",
    "sowing_date_dict = sowing_date_df.set_index([\"Occ\", \"Loc_no\", \"Cycle\"])[[\"Year\", \"Month\", \"Day\"]].to_dict(orient=\"index\")\n",
    "\n",
    "# Define a function to lookup (Year, Month, Day) data based on (Occ, Loc_no, Cycle)\n",
    "def get_date(row):\n",
    "    key = (row[\"Occ\"], row[\"Loc_no\"], row[\"Cycle\"])\n",
    "    date_data = sowing_date_dict.get(key)\n",
    "    if date_data:\n",
    "        return int(date_data[\"Year\"]), int(date_data[\"Month\"]), int(date_data[\"Day\"])\n",
    "    return None, None, None\n",
    "\n",
    "# Apply the get_date function using the apply method to generate SowYear, SowMonth, and SowDay columns\n",
    "allpheno_df[[\"SowYear\", \"SowMonth\", \"SowDay\"]] = allpheno_df.apply(get_date, axis=1, result_type=\"expand\")\n",
    "\n",
    "allpheno_df.to_csv(\"output/AllWithGidDropMissingValuesSow.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "allpheno_df = pd.read_csv(\"output/AllWithGidDropMissingValuesSow.csv\")\n",
    "\n",
    "# Read HARVEST_FINISHING_DATE.csv file and handle duplicate rows\n",
    "sowing_date_df = pd.read_csv(\"source_data/HARVEST_FINISHING_DATE.csv\", dtype={\"Year\": int, \"Month\": int, \"Day\": int})\n",
    "sowing_date_df.drop_duplicates(subset=[\"Occ\", \"Loc_no\", \"Cycle\"], keep=\"first\", inplace=True)\n",
    "\n",
    "# Convert HARVEST_FINISHING_DATE.csv data to a dictionary with (Occ, Loc_no, Cycle) as key and (Year, Month, Day) as values\n",
    "sowing_date_dict = sowing_date_df.set_index([\"Occ\", \"Loc_no\", \"Cycle\"])[[\"Year\", \"Month\", \"Day\"]].to_dict(orient=\"index\")\n",
    "\n",
    "# Define a function to lookup (Year, Month, Day) data based on (Occ, Loc_no, Cycle)\n",
    "def get_date(row):\n",
    "    key = (row[\"Occ\"], row[\"Loc_no\"], row[\"Cycle\"])\n",
    "    date_data = sowing_date_dict.get(key)\n",
    "    if date_data:\n",
    "        return int(date_data[\"Year\"]), int(date_data[\"Month\"]), int(date_data[\"Day\"])\n",
    "    return None, None, None\n",
    "\n",
    "# Apply the get_date function using the apply method to generate HarYear, HarMonth, and HarDay columns\n",
    "allpheno_df[[\"HarYear\", \"HarMonth\", \"HarDay\"]] = allpheno_df.apply(get_date, axis=1, result_type=\"expand\")\n",
    "\n",
    "allpheno_df = allpheno_df.dropna()\n",
    "\n",
    "# Ensure SowYear, SowMonth, SowDay, HarYear, HarMonth, HarDay columns are integers\n",
    "allpheno_df[[\"SowYear\", \"SowMonth\", \"SowDay\", \"HarYear\", \"HarMonth\", \"HarDay\"]] = allpheno_df[[\"SowYear\", \"SowMonth\", \"SowDay\", \"HarYear\", \"HarMonth\", \"HarDay\"]].astype(int)\n",
    "\n",
    "# Create SowDate and HarDate columns using datetime\n",
    "allpheno_df['SowDate'] = pd.to_datetime(allpheno_df[['SowYear', 'SowMonth', 'SowDay']].astype(str).agg('-'.join, axis=1))\n",
    "allpheno_df['HarDate'] = pd.to_datetime(allpheno_df[['HarYear', 'HarMonth', 'HarDay']].astype(str).agg('-'.join, axis=1))\n",
    "\n",
    "# Calculate the date difference in days and store it in the \"Days\" column\n",
    "allpheno_df['Days'] = (allpheno_df['HarDate'] - allpheno_df['SowDate']).dt.days\n",
    "\n",
    "# Optionally remove intermediate columns SowDate and HarDate\n",
    "allpheno_df.drop(['SowDate', 'HarDate'], axis=1, inplace=True)\n",
    "\n",
    "# 保存结果到另一个CSV文件\n",
    "allpheno_df.to_csv(\"output/AllWithGidDropMissingValuesSowHar.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to read values from a txt file and store them as a set\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as txt_file:\n",
    "        values = txt_file.read().split()\n",
    "    return set(values)\n",
    "\n",
    "# Function to filter rows in a CSV file based on Gid values from the txt file\n",
    "def filter_csv_file(input_file, txt_file, output_file):\n",
    "    values_set = read_txt_file(txt_file)\n",
    "\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    filtered_df = df[df['Gid'].isin(values_set)]\n",
    "\n",
    "    filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = \"output/AllWithGidDropMissingValuesSowHar.csv\"\n",
    "    txt_file = \"../2_Geno/output/genotype_ID.txt\" \n",
    "    output_csv_file = \"output/AllWithGidDropMissingValuesSowHarFilteredGid.csv\"\n",
    "\n",
    "    filter_csv_file(input_csv_file, txt_file, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_before2020 = 'output/AllWithGidDropMissingValuesSowHarFilteredGid.csv'\n",
    "file_weather_data = '../3_Env/output/IWIN_Weather_AgERA5_20210211.csv'\n",
    "output_file = 'output/AllWithGidDropMissingValuesSowHarFilteredGidLoc.csv'\n",
    "\n",
    "data_before2020 = pd.read_csv(file_before2020)\n",
    "data_weather = pd.read_csv(file_weather_data)\n",
    "\n",
    "locations_before2020 = set(data_before2020['Loc_no'])\n",
    "locations_weather_data = set(data_weather['location'])\n",
    "\n",
    "cleaned_data = data_before2020[data_before2020['Loc_no'].isin(locations_weather_data)]\n",
    "\n",
    "deleted_data = data_before2020[~data_before2020['Loc_no'].isin(locations_weather_data)]\n",
    "\n",
    "if not deleted_data.empty:\n",
    "    print(f\"Deleted {len(deleted_data)} rows where 'Loc_no' is not in 'location'.\")\n",
    "    deleted_file_name = 'output/deleted_data.csv'\n",
    "    deleted_data.to_csv(deleted_file_name, index=False)\n",
    "    print(f\"Deleted rows saved to file: {deleted_file_name}\")\n",
    "else:\n",
    "    print(\"All 'Loc_no' values exist in 'location'. No rows were deleted.\")\n",
    "\n",
    "if not cleaned_data.empty:\n",
    "    print(f\"Cleaned {len(cleaned_data)} rows where 'Loc_no' exists in 'location'.\")\n",
    "    cleaned_data.to_csv(output_file, index=False)\n",
    "    print(f\"Cleaned data saved to file: {output_file}\")\n",
    "else:\n",
    "    print(\"No rows to clean. All 'Loc_no' values exist in 'location'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"删除不合理的值:1 收获日大于2020的(不掌握2020后的天气数据);2.生长周期大于300天的;3.表型值大于20.\"\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLoc.csv')\n",
    "\n",
    "# Define conditions for filtering\n",
    "condition = (df['Days'] > 270) | (df['HarYear'] >= 2020 ) | (df['Value'] == \"-\" )  | (df['Value'] >= 20 )\n",
    "\n",
    "filtered_df = df[condition]\n",
    "\n",
    "filtered_df.to_csv('output/Unormal.csv', index=False)\n",
    "\n",
    "# Remove rows that meet the conditions\n",
    "df.drop(filtered_df.index, inplace=True)\n",
    "\n",
    "df.to_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormal.csv')\n",
    "\n",
    "# Identify duplicate rows, keeping the first occurrence and deleting the rest\n",
    "df_duplicates = df[df.duplicated(keep='first')]\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "# Print the first duplicate row\n",
    "if not df_duplicates.empty:\n",
    "    print(\"The first duplicate row:\")\n",
    "    print(df_duplicates.head())\n",
    "\n",
    "# Save the removed duplicate rows to a new CSV file\n",
    "df_duplicates.to_csv('output/Duplicate.csv', index=False)\n",
    "print(\"The removed duplicate rows have been saved to Duplicate.csv.\")\n",
    "\n",
    "# Save the updated data back to the original file\n",
    "df.to_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv', index=False)\n",
    "print(\"The updated data has been saved to AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"提取去除重复后的环境型\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv')\n",
    "\n",
    "# Combine three columns into a new column\n",
    "df['Occ_Loc_no_Cycle'] = df['Occ'].astype(str) + '_' + df['Loc_no'].astype(str) + '_' + df['Cycle'].astype(str)\n",
    "\n",
    "df = df.drop_duplicates(subset='Occ_Loc_no_Cycle', keep='first')\n",
    "\n",
    "df.to_csv('output/UniqueOccLocCycle.csv', columns=['Occ_Loc_no_Cycle'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"提取去除重复后的基因型\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv')\n",
    "\n",
    "# Extract the Gid column and keep the first occurrence of each unique value\n",
    "unique_gid_series = df['Gid'].drop_duplicates(keep='first')\n",
    "\n",
    "new_df = pd.DataFrame({'Gid': unique_gid_series})\n",
    "\n",
    "new_df.to_csv('output/UniqueGid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIGS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
