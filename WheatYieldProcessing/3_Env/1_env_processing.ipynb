{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Process for Wheat Yield Environment Data**\n",
    "\n",
    "This step requires the file IWIN_Weather_AgERA5_20210211.txt to be located in the source_data folder, And then execute the notebook 1_env_processing.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def txt_to_csv(input_file, output_file):\n",
    "    with open(input_file, 'r') as txt_file:\n",
    "        lines = txt_file.readlines()\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        for line in lines:\n",
    "            row = line.strip().split(',')\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_txt_file = \"source_data/IWIN_Weather_AgERA5_20210211.txt\"\n",
    "    output_csv_file = \"output/IWIN_Weather_AgERA5_20210211.csv\" \n",
    "\n",
    "    txt_to_csv(input_txt_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output/IWIN_Weather_AgERA5_20210211.csv')\n",
    "\n",
    "df_filtered = df[df['Year'] >= 2002]\n",
    "\n",
    "df_filtered.to_csv('output/IWIN_Weather_AgERA5_2003-2021.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "weather_data_file = 'output/IWIN_Weather_AgERA5_2003-2021.csv'\n",
    "merged_data_file = '../1_Pheno/output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv'\n",
    "\n",
    "\n",
    "weather_data = pd.read_csv(weather_data_file)\n",
    "merged_data = pd.read_csv(merged_data_file)\n",
    "\n",
    "unique_locations = merged_data['Loc_no'].drop_duplicates()\n",
    "\n",
    "filtered_weather_data = weather_data[weather_data['location'].isin(unique_locations)]\n",
    "\n",
    "filtered_weather_data.to_csv('output/IWIN_Weather_AgERA5_2003-2021_Trimed.csv', index=False)\n",
    "\n",
    "# Print a message with the number of rows and unique locations retained\n",
    "print(f\"筛选完成！共保留 {len(filtered_weather_data)} 行数据，涉及 {len(unique_locations)} 个唯一地点。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "weather_data_file = 'output/IWIN_Weather_AgERA5_2003-2021_Trimed.csv'\n",
    "merged_data_file = '../1_Pheno/output/AllWithGidDropMissingValuesSowHarFilteredGidLocUnormalNoDuplicated.csv'\n",
    "\n",
    "weather_data = pd.read_csv(weather_data_file)\n",
    "merged_data = pd.read_csv(merged_data_file)\n",
    "\n",
    "# Optimize date field merging\n",
    "def combine_date(year, month, day):\n",
    "    return pd.to_datetime({'year': year, 'month': month, 'day': day})\n",
    "\n",
    "# Convert sowing and harvest dates to datetime format\n",
    "merged_data['SowDate'] = combine_date(\n",
    "    merged_data['SowYear'], merged_data['SowMonth'], merged_data['SowDay']\n",
    ")\n",
    "merged_data['HarDate'] = combine_date(\n",
    "    merged_data['HarYear'], merged_data['HarMonth'], merged_data['HarDay']\n",
    ")\n",
    "\n",
    "# Convert weather data date fields to datetime format\n",
    "weather_data['Date'] = pd.to_datetime(\n",
    "    weather_data[['Year', 'Month', 'Day']]\n",
    ")\n",
    "\n",
    "# Iterate through the main dataset\n",
    "results = []\n",
    "\n",
    "for index, row in merged_data.iterrows():\n",
    "    print(index)\n",
    "    loc_no = row['Loc_no']\n",
    "    sow_date = row['SowDate']\n",
    "    har_date = row['HarDate']\n",
    "    days_before_sow = sow_date - timedelta(days=14)\n",
    "\n",
    "    # Filter weather data\n",
    "    weather_subset = weather_data[\n",
    "        (weather_data['location'] == loc_no) &\n",
    "        (weather_data['Date'] >= days_before_sow) &\n",
    "        (weather_data['Date'] <= har_date)\n",
    "    ]\n",
    "\n",
    "    if weather_subset.empty:\n",
    "        print(f\"Warning: No weather data found for Loc_no={loc_no}, SowDate={sow_date}, HarvestDate={har_date}\")\n",
    "        continue\n",
    "\n",
    "    # Group by every 7 days and calculate average, use direct average for less than 7 days\n",
    "    weather_subset = weather_subset.sort_values('Date')\n",
    "    weather_subset['WeekIndex'] = (weather_subset['Date'] - days_before_sow).dt.days // 7\n",
    "    weekly_weather = weather_subset.groupby('WeekIndex').mean(numeric_only=True).round(2)\n",
    "\n",
    "    # Select relevant weather columns\n",
    "    selected_weather_columns = [\n",
    "        'Precipitation [mm]', 'Relative Humidity max [%]', \n",
    "        'Relative Humidity min [%]', 'Shortwave Radiation [MJ/m2/d]', \n",
    "        'TemperatureMax [C]', 'TemperatureMin [C]', \n",
    "        'Vapor Pressure Deficit max [kPa]', \n",
    "        'Wind Speed 2m [m/s]', 'Wind Speed 10m [m/s]'\n",
    "    ]\n",
    "\n",
    "    flattened_data = {}\n",
    "    for week_idx, week_data in weekly_weather.iterrows():\n",
    "        for col in selected_weather_columns:\n",
    "            flattened_data[f\"Week{week_idx+1}_{col}\"] = week_data[col]\n",
    "\n",
    "    results.append({**row.to_dict(), **flattened_data})\n",
    "\n",
    "final_df = pd.DataFrame(results)\n",
    "final_df.to_csv('output/YieldWeeklyWeather.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "data = pd.read_csv('output/YieldWeeklyWeather.csv')\n",
    "\n",
    "# Extract all columns starting from the specified column\n",
    "start_column = \"Week1_Precipitation [mm]\"\n",
    "subset_data = data.loc[:, start_column:]\n",
    "\n",
    "# Fill missing values with 0\n",
    "subset_data = subset_data.fillna(0)\n",
    "\n",
    "# Extract values and store them as a 2D array\n",
    "processed_data = []\n",
    "for index, row in subset_data.iterrows():\n",
    "\n",
    "    row_data = row.dropna().values  # Extract the numeric values\n",
    "    if len(row_data) > 0:\n",
    "        # Ensure the data length is a multiple of 9\n",
    "        if len(row_data) % 9 == 0:\n",
    "            row_matrix = np.array(row_data).reshape(-1, 9)  # Convert to a 2D array\n",
    "            processed_data.append(row_matrix)\n",
    "        else:\n",
    "            print(index+1)\n",
    "            print(row_data.shape)\n",
    "            \n",
    "data_array = np.array(processed_data)\n",
    "\n",
    "with open('output/YieldWeeklyWeather.pkl', 'wb') as f:\n",
    "    pickle.dump(data_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized weather\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the 2D array from the PKL file\n",
    "with open('output/YieldWeeklyWeather.pkl', 'rb') as file:\n",
    "    matrix = pickle.load(file)\n",
    "\n",
    "N, W, H = matrix.shape\n",
    "\n",
    "# Reshape the matrix to a 2D array for processing\n",
    "matrix = matrix.reshape(N * W, H)\n",
    "\n",
    "# Record the row indices where all values are zero\n",
    "zero_rows = np.all(matrix == 0, axis=1)\n",
    "\n",
    "# Replace all-zero rows with np.nan\n",
    "matrix[zero_rows] = np.nan\n",
    "\n",
    "# Compute the mean and standard deviation for each column, ignoring np.nan values\n",
    "column_means = np.nanmean(matrix, axis=0)\n",
    "column_stds = np.nanstd(matrix, axis=0)\n",
    "\n",
    "# Initialize a new matrix to store the normalized data\n",
    "normalized_matrix = np.zeros_like(matrix)\n",
    "\n",
    "# Standardize each column\n",
    "for i in range(matrix.shape[1]):\n",
    "    if column_stds[i] != 0.0:  # Exclude columns with all zeros\n",
    "        normalized_matrix[:, i] = (matrix[:, i] - column_means[i]) / column_stds[i]\n",
    "\n",
    "# Replace np.nan values in the normalized matrix with 0\n",
    "normalized_matrix[np.isnan(normalized_matrix)] = 0\n",
    "\n",
    "normalized_matrix = normalized_matrix.reshape(N, W, H)\n",
    "\n",
    "with open('output/YieldWeeklyWeatherNormalized.pkl', 'wb') as file:\n",
    "    pickle.dump(normalized_matrix, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIGS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
